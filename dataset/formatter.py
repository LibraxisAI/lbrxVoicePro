"""MOSHI/MIMI format converter"""

import json
from pathlib import Path
from typing import Dict, Any, List
import numpy as np


class MoshiMimiFormatter:
    """Format datasets for MOSHI/MIMI training"""
    
    @staticmethod
    def to_moshi_format(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Convert to MOSHI training format"""
        
        moshi_data = {
            "version": "1.0",
            "type": "speech_dataset",
            "metadata": {
                "languages": list(set(s.get("language", "pl") for s in samples)),
                "speakers": list(set(s.get("speaker_id", "unknown") for s in samples)),
                "total_hours": sum(s.get("duration", 0) for s in samples) / 3600,
                "sample_rate": 24000,  # MOSHI uses 24kHz
            },
            "utterances": []
        }
        
        for sample in samples:
            utterance = {
                "id": sample["id"],
                "audio_path": f"audio/{sample['audio_file']}",
                "text": sample["text"],
                "duration": sample["duration"],
                "speaker": sample.get("speaker_id", "unknown"),
                "language": sample.get("language", "pl"),
                # MOSHI specific fields
                "semantic_tokens": None,  # Will be generated by MOSHI
                "acoustic_tokens": None,  # Will be generated by MOSHI
            }
            
            # Add word-level alignments if available
            if "segments" in sample and sample["segments"]:
                utterance["alignments"] = [
                    {
                        "word": seg.get("text", ""),
                        "start": seg.get("start", 0),
                        "end": seg.get("end", 0)
                    }
                    for seg in sample["segments"]
                    if "start" in seg and "end" in seg
                ]
            
            moshi_data["utterances"].append(utterance)
        
        return moshi_data
    
    @staticmethod
    def to_mimi_format(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Convert to MIMI codec format"""
        
        mimi_data = {
            "version": "1.0",
            "codec": "mimi",
            "config": {
                "sample_rate": 24000,
                "channels": 1,
                "frame_rate": 12.5,  # MIMI uses 12.5 fps
                "codebook_size": 2048,
                "num_codebooks": 8,
            },
            "data": []
        }
        
        for sample in samples:
            mimi_sample = {
                "id": sample["id"],
                "audio": {
                    "path": f"audio/{sample['audio_file']}",
                    "duration": sample["duration"],
                    "sample_rate": sample.get("sample_rate", 16000)
                },
                "text": {
                    "content": sample["text"],
                    "language": sample.get("language", "pl")
                },
                "metadata": {
                    "speaker": sample.get("speaker_id", "unknown"),
                    "timestamp": sample.get("timestamp", "")
                }
            }
            
            mimi_data["data"].append(mimi_sample)
        
        return mimi_data
    
    @staticmethod
    def prepare_for_csm_training(moshi_data: Dict[str, Any], 
                                output_dir: Path) -> None:
        """Prepare dataset for CSM-MLX training"""
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create train/val/test splits
        utterances = moshi_data["utterances"]
        n_samples = len(utterances)
        
        # 80/10/10 split
        train_size = int(0.8 * n_samples)
        val_size = int(0.1 * n_samples)
        
        np.random.shuffle(utterances)
        
        splits = {
            "train": utterances[:train_size],
            "val": utterances[train_size:train_size + val_size],
            "test": utterances[train_size + val_size:]
        }
        
        # Save splits
        for split_name, split_data in splits.items():
            split_file = output_dir / f"{split_name}.json"
            
            with open(split_file, "w", encoding="utf-8") as f:
                json.dump({
                    "version": moshi_data["version"],
                    "metadata": moshi_data["metadata"],
                    "utterances": split_data
                }, f, ensure_ascii=False, indent=2)
                
            print(f"✅ Saved {split_name} split: {len(split_data)} samples")
        
        # Create manifest for CSM
        manifest = {
            "dataset_name": "lbrxVoicePro_Polish",
            "language": "pl",
            "splits": {
                "train": str(output_dir / "train.json"),
                "val": str(output_dir / "val.json"),
                "test": str(output_dir / "test.json")
            },
            "audio_dir": str(output_dir.parent / "audio"),
            "preprocessing": {
                "sample_rate": 24000,
                "normalize": True,
                "trim_silence": True
            }
        }
        
        with open(output_dir / "manifest.json", "w") as f:
            json.dump(manifest, f, indent=2)
            
        print(f"✅ Dataset prepared for CSM training at {output_dir}")